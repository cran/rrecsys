<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{10. Evaluation}
-->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rrecsys)
```

## Evaluation

The evaluation module is based on the _k-fold cross-validation_ method. A _stratified_ random selection procedure is applied when dividing the _rated items_ of each user into $k$ folds such that each user is uniformly represented in each fold, i.e. the number of ratings of each user in any fold differs at most by one. For k-fold cross validation each of the $k$ disjoint fractions of the ratings are used $k-1$ times for training (i.e. $\subset R_{train}$) and once for testing (i.e. $\subset R_{test}$). Practically, ratings in $R_{test}$ are set as missing in the original dataset and predictions/recommendations are compared to $R_{test}$ to compute the  performance measures.

We included many popular performance metrics. These are mean absolute error (MAE), root mean square error(RMSE), Precision, Recall, F1, True and False Positives, True and False Negatives, normalized discounted cumulative gain (NDCG), rank score, area under the ROC curve (AUC) and catalog coverage. RMSE and MAE metrics are computed according to their two variants, user-based vs. global. The user-based variant weights each user uniformly by computing the metric for each user separately and averaging over all users while in the global variant users with larger test sets have more weight.

To evaluate via rrecsys we must start my creating the evaluation model:

```{r, eval=FALSE}
e <- evalModel(smallML, folds = 5)
```
The same model may be used on different algorithms. To evaluate a prediction using this model:
```{r, eval=FALSE}
evalPred(e, "ibknn", neigh = 5)
```

An evaluation on a recommendation:
```{r, eval=FALSE}
evalRec(e, "funk", k = 5, lambda = 0.001, goodRating = 3, topN = 3)
```
The _evalPred_ and _evalRec_ arguments correspond to the arguments of the algorithm that we want to evaulate. _evalRec_, obviosly, has the _topN_ argument and two additional arguments. The _goodRating_ that defines a threshold to distinguish between a negative rating and a positive rating. The _alpha_ argument is the ranking half-life for the rank score metric.

To compute the AUC:
```{r, eval=FALSE}
getAUC(e, "globalAv")
```
